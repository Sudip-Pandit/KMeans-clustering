### 1. Data Preprocessing and Exploration
#### First, let's preprocess the data and explore it to understand its structure.
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# Load your dataset
df = pd.read_csv("your_dataset.csv")

# Assume all columns except 'id' are features for clustering
X = df.drop('id', axis=1)

# Standardize the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Optionally, reduce dimensionality for visualization or to speed up clustering
pca = PCA(n_components=2)  # Adjust n_components based on your dataset
X_pca = pca.fit_transform(X_scaled)

## 2. Applying KMeans Clustering
#### Next, apply KMeans clustering to the processed data.

from sklearn.cluster import KMeans
import numpy as np

# Choose the number of clusters (k) and fit KMeans
k = 3  # Example number of clusters
kmeans = KMeans(n_clusters=k, random_state=42)
clusters = kmeans.fit_predict(X_scaled)

# Optionally, add cluster assignments to the original dataframe for analysis
df['cluster'] = clusters

## 3. Evaluating Clusters
#### Evaluate how well the data points have been grouped into clusters.
import matplotlib.pyplot as plt
import seaborn as sns

# Visualizing the clusters (if PCA was applied)
plt.figure(figsize=(8, 6))
sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue=clusters, palette='viridis')
plt.title('Data points in PCA-reduced space colored by cluster')
plt.show()

# Calculate silhouette score to evaluate clustering
from sklearn.metrics import silhouette_score
silhouette_avg = silhouette_score(X_scaled, clusters)
print(f"Silhouette Score: {silhouette_avg:.3f}")

## 4. Save the Model
#### Save the KMeans model using joblib for later use.
import joblib

# Save the KMeans model
joblib.dump(kmeans, 'kmeans_model.joblib')

## 5. Deploy the Model with Flask
#### Create a simple Flask application that loads the KMeans model and assigns new data points to clusters.
from flask import Flask, request, jsonify
import joblib

app = Flask(__name__)

# Load the trained KMeans model
kmeans_model = joblib.load('kmeans_model.joblib')

@app.route('/predict_cluster', methods=['POST'])
def predict_cluster():
    data = request.get_json(force=True)
    # Ensure the input data matches the feature shape used during training
    prediction = kmeans_model.predict([data['features']])
    return jsonify({'cluster': prediction.tolist()})

if __name__ == '__main__':
    app.run(port=5000, debug=True)

## 6. Testing the Deployed Model
#### You can test the deployed model similarly by sending a POST request with feature values to the /predict_cluster endpoint.
#### Important Notes
#### Dataset and Features: Adjust the script to fit your specific dataset, particularly the path to your CSV file and the features used for clustering.
#### Number of Clusters: The choice of k (number of clusters) in KMeans is crucial. Use methods like the Elbow method or the Silhouette score to determine a good value for k.
#### Deployment: The Flask app provided is for demonstration purposes. For production, consider security, scalability, and deployment best practices.

